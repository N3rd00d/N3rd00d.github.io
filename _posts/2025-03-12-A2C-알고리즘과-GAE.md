---
title: "A2C 알고리즘과 GAE"
description: "A2C(Advantage Actor-Critic) 알고리즘은 정책 그래디언트 기법과 가치 함수 근사를 결합하여 높은 안정성과 효율성을 제공하는 강화학습 알고리즘이다."
date: 2025-03-12T18:59:07.842Z
tags: ["ActorCritic","GAE","a2c","advantage","강화학습","딥러닝","인공지능"]
---
# A2C 알고리즘

A2C(Advantage Actor-Critic) 알고리즘은 정책 그래디언트 기법과 가치 함수 근사를 결합하여 높은 안정성과 효율성을 제공하는 강화학습 알고리즘이다. 이 알고리즘은 REINFORCE 알고리즘의 높은 분산 문제를 해결하고, 심층신경망의 파라미터($\theta$)를 효과적으로 조정하여 정책 최적화를 달성한다.

## A2C의 주요 장점

REINFORCE 알고리즘에는 두 가지 단점이 있다:
- 그래디언트 추정의 분산이 매우 커서 학습이 불안정하고 많은 샘플이 필요하다.
- 에피소드가 완전히 끝나야만 정책을 업데이트할 수 있어 학습 속도가 느리다.

A2C는 이러한 문제를 해결한다. Advantage 함수를 사용하여 그래디언트 추정의 분산을 크게 줄이고, TD(시간차) 학습을 통해 에피소드가 끝나기 전에도 매 단계마다 정책을 업데이트할 수 있다. 이 덕분에 A2C는 더 안정적이고 효율적인 학습이 가능하며, 복잡한 환경에서도 좋은 성능을 발휘한다.

## Actor-Critic 구조

A2C는 두 개의 주요 구성 요소로 이루어져 있다:

1. **Actor(정책 네트워크)**: 주어진 상태에서 행동을 선택하는 정책 $\pi_\theta(a|s)$를 학습한다.
2. **Critic(가치 네트워크)**: 상태의 가치를 평가하는 가치 함수 $V_\phi(s)$를 학습한다.

이러한 구조는 다음과 같은 이점을 제공한다:
- Actor는 Critic의 평가를 바탕으로 정책을 개선한다.
- Critic은 실제 경험을 통해 상태 가치를 정확하게 추정한다.
- 두 네트워크의 상호작용으로 학습 안정성과 샘플 효율성이 향상된다.

## Advantage 함수

A2C 알고리즘의 핵심은 Advantage 함수 $A(s,a)$를 사용하는 것이다. Advantage 함수는 특정 행동이 평균적인 행동보다 얼마나 더 좋은지 측정한다.

$$
A(s,a) = Q(s,a) - V(s)
$$

- $Q(s,a)$: 상태 $s$에서 행동 $a$를 취한 후의 기대 리턴
- $V(s)$: 상태 $s$의 기대 가치

Advantage 함수는 다음과 같이 근사할 수 있다:

$$
A(s_t,a_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)
$$

이 표현식은 시간차 오차(TD error)라고도 하며, 현재 상태에서 취한 행동의 상대적 가치를 나타낸다.

## 정책 그래디언트 업데이트

A2C 알고리즘에서 정책 파라미터 $\theta$의 업데이트는 다음 수식에 따라 이루어진다:

$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ A(s,a) \nabla_\theta \log \pi_\theta(a|s) \right]
$$

이는 REINFORCE 알고리즘과 유사하지만, 중요한 차이점은 리턴 $G_t$ 대신 Advantage 함수 $A(s,a)$를 사용한다는 점이다. 이를 통해 그래디언트 추정의 분산을 크게 줄일 수 있다.

실제 구현에서 정책 그래디언트 업데이트는 다음과 같다:

$$
\theta \leftarrow \theta + \alpha \sum_t A(s_t,a_t) \nabla_\theta \log \pi_\theta(a_t|s_t)
$$

## 가치 함수 업데이트

A2C에서 Critic 네트워크는 시간차(TD) 학습을 통해 가치 함수를 갱신한다. TD 학습은 다음과 같은 특징을 가진다:

- 에피소드 종료를 기다리지 않고 매 스텝마다 가치 함수 갱신
- 실시간 학습으로 샘플 효율성 향상

Critic 네트워크의 파라미터 $\phi$는 다음과 같이 업데이트된다:

$$\phi \leftarrow \phi - \beta \nabla_\phi \sum_t \left( V_\phi(s_t) - (r_t + \gamma V_\phi(s_{t+1})) \right)^2$$

여기서 $\beta$는 Critic의 학습률이고, 목표는 예측된 가치 $V_\phi(s_t)$와 TD 타겟 $r_t + \gamma V_\phi(s_{t+1})$ 사이의 제곱 오차를 최소화하는 것이다. 이 방식으로 Critic은 상태 가치를 점진적으로 정확하게 추정할 수 있게 된다.

### TD(시간차) 학습

TD 학습은 현재 상태의 가치를 다음 상태의 추정 가치와 즉각적인 보상을 통해 갱신하는 방법이다. 몬테카를로 방식이 에피소드 전체의 실제 보상을 사용하여 더 정확한 추정을 제공할 수 있는 반면, TD 학습은 즉각적인 업데이트가 가능하여 실시간 학습에 적합하다.

## A2C 알고리즘 의사코드

다음은 A2C 알고리즘의 이해를 돕기 위해 작성한 간결한 의사코드이다:

```
초기화:
- 정책 네트워크(Actor) π_θ(a|s) 및 가치 네트워크(Critic) V_φ(s) 초기화
- 학습률 α_θ, α_φ 및 할인 인자 γ 설정

for 에피소드 = 1, 2, ..., N do
  상태 s_0 초기화
  
  for t = 0, 1, 2, ... 종료 시점까지 do
    정책 π_θ(a|s_t)에 따라 행동 a_t 선택
    행동 a_t 실행, 보상 r_t 받고 다음 상태 s_{t+1} 관찰
    
    # TD 학습을 통한 실시간 업데이트
    # Advantage 계산
    A(s_t, a_t) = r_t + γ V_φ(s_{t+1}) - V_φ(s_t)
    
    # Actor 업데이트: Advantage를 이용해 정책 개선
    θ ← θ + α_θ A(s_t, a_t) ∇_θ log π_θ(a_t|s_t)
    
    # Critic 업데이트: 가치 함수를 TD 목표에 맞춤
    φ ← φ - α_φ ∇_φ (V_φ(s_t) - (r_t + γ V_φ(s_{t+1})))²
  end for
end for
```

1. 각 상태-행동 쌍의 Advantage를 TD 오차로 실시간 계산한다.
2. 매 단계마다 즉시 Actor와 Critic 네트워크를 업데이트한다.
3. 에피소드가 끝날 때까지 기다리지 않고 실시간으로 학습한다.

## GAE (Generalized Advantage Estimation)

A2C 알고리즘의 성능을 더욱 향상시키기 위해 Advantage 함수 대신 GAE(Generalized Advantage Estimation)를 사용할 수 있다. GAE는 어드밴티지 함수 추정 시 발생하는 분산과 편향 사이의 균형을 조절하는 방법이다.

### GAE의 핵심 개념

GAE는 다음과 같은 수식으로 정의된다:

$$\hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

여기서:
- $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$: TD 오류
- $\gamma$: 할인율 (미래 보상의 중요도)
- $\lambda$: GAE 파라미터 (0~1 사이의 값)

이 수식은 본질적으로 TD 오류($\delta_t$)의 지수가중평균으로 현재부터 미래까지의 여러 시점에서 계산된 TD 오류에 지수적으로 감소하는 가중치 $(\gamma\lambda)^l$를 부여하여 가중 합을 구한다.

$\lambda$ 값에 따라 GAE는 다음과 같은 특성을 보인다:

- $\lambda = 0$: 한 시점의 TD 오류만 사용 ($\hat{A}_t = \delta_t$)
- $\lambda = 1$: 모든 미래 시점의 TD 오류를 감쇄 없이 합산 (몬테카를로 리턴과 동일)
- $0 < \lambda < 1$: 여러 시점의 TD 오류를 지수적으로 감소하는 가중치로 평균화

$\lambda$ 값을 조절함으로써 다양한 시간 스케일의 정보를 적절히 혼합할 수 있다. 낮은 $\lambda$ 값은 단기적 정보에 더 큰 가중치를 부여하여 분산을 줄이지만 편향을 증가시킨다. 반면, 높은 $\lambda$ 값은 장기적 정보를 더 많이 반영하여 편향을 줄이지만 분산이 커진다. 실용적으로는 $\lambda$ = 0.9~0.99 범위의 값이 많이 사용되어 장기적 신호를 충분히 반영하면서도 분산을 적절히 제어한다.

### GAE의 수학적 유도

GAE는 다양한 길이의 n-스텝 리턴을 지수가중평균하여 얻을 수 있다. 이를 수학적으로 유도하는 과정은 다음과 같다:

1. **n-스텝 리턴 정의**:
   n-스텝 TD 타겟은 n번째 상태까지의 실제 보상을 사용하고, 그 이후는 가치 함수로 대체한다:
   
   $$G_t^{(n)} = r_t + \gamma r_{t+1} + ... + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$
   
   이를 이용한 n-스텝 어드밴티지 추정은 다음과 같다:
   
   $$\hat{A}_t^{(n)} = G_t^{(n)} - V(s_t)$$

2. **n-스텝 리턴을 TD 오류로 표현**:
   n-스텝 리턴은 1-스텝 TD 오류들의 합으로 표현할 수 있다:
   
   $$G_t^{(n)} - V(s_t) = \sum_{l=0}^{n-1} \gamma^l \delta_{t+l}$$
   
   여기서 $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$는 1-스텝 TD 오류이다.

3. **다양한 길이의 n-스텝 리턴 지수 가중 조합**:
   GAE는 다양한 길이의 n-스텝 리턴을 λ에 따라 지수적으로 가중치를 부여해 조합한다:
   
   $$\hat{A}_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} \hat{A}_t^{(n)}$$
   
   여기서 $(1-\lambda)$ 항은 무한 등비급수 $\sum_{n=1}^{\infty} \lambda^{n-1} = \frac{1}{1-\lambda}$의 합을 1로 정규화하기 위한 항이다. 예를 들면 λ가 0.9일 경우 가중치 합이 10이 되는 문제를 해결하여, 어드밴티지 추정치의 스케일을 일관되게 유지하고 안정적인 학습을 가능하게 한다.

4. **TD 오류의 지수가중평균으로 변환**:
   위 식을 전개하면 다음과 같다:
   
   $$\hat{A}_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} \sum_{l=0}^{n-1} \gamma^l \delta_{t+l}$$
   
   수열의 순서를 교환하고 항을 정리하면:
   
   $$\hat{A}_t^{\lambda} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$


 - [참고1. GAE식의 유도](https://youtu.be/cIyXYYdZIsk?t=1676)
 - [참고2. n스텝 리턴합으로 GAE 유도](https://hiddenbeginner.github.io/Deep-Reinforcement-Learnings/book/Chapter2/8-gae.html)

### GAE의 이점

GAE를 A2C에 적용하면 다음과 같은 이점이 있다:
- 어드밴티지 추정의 분산을 효과적으로 줄여 학습 안정성이 향상된다.
- $\lambda$ 파라미터 조정을 통해 문제에 맞는 최적의 편향-분산 균형을 찾을 수 있다.
- 복잡한 환경에서도 정책 그래디언트가 안정적으로 업데이트된다.

### A2C와 GAE의 결합

A2C 알고리즘에 GAE를 적용하면 의사코드는 다음과 같이 수정된다:

```
초기화:
- 정책 네트워크(Actor) π_θ(a|s) 및 가치 네트워크(Critic) V_φ(s) 초기화
- 학습률 α_θ, α_φ, 할인 인자 γ 및 GAE 파라미터 λ 설정

for 에피소드 = 1, 2, ..., N do
  상태 s_0 초기화
  경험 버퍼 초기화 (상태, 행동, 보상, 다음 상태 저장용)
  
  for t = 0, 1, 2, ... 종료 시점까지 do
    정책 π_θ(a|s_t)에 따라 행동 a_t 선택
    행동 a_t 실행, 보상 r_t 받고 다음 상태 s_{t+1} 관찰
    경험 (s_t, a_t, r_t, s_{t+1}) 버퍼에 저장
    
    # 일정 스텝(n)마다 또는 에피소드 종료 시 TD 학습 수행
    if (t % n == 0) 또는 에피소드 종료 then
      # 각 상태에 대한 TD 오류 계산
      for i = 버퍼 내 모든 인덱스 do
        δ_i = r_i + γV_φ(s_{i+1}) - V_φ(s_i)  # TD 오류
      end for
      
      # GAE를 이용한 어드밴티지 계산 (역방향)
      A_last = 0
      for i = 버퍼의 마지막 인덱스부터 첫 인덱스까지 do
        A_i = δ_i + γλA_{i+1}  # GAE 계산
        버퍼[i]에 A_i 저장
      end for
      
      # 계산된 어드밴티지로 Actor와 Critic 동시에 업데이트
      θ ← θ + α_θ ∇_θ Σ_i A_i log π_θ(a_i|s_i)
      φ ← φ - α_φ ∇_φ Σ_i (V_φ(s_i) - (r_i + γV_φ(s_{i+1})))²
      
      # 버퍼 비우기 (선택적)
      경험 버퍼 초기화
    end if
  end for
end for
```

이 의사코드에서 GAE 계산에 대한 설명:

GAE 수식 $\hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$을 효율적으로 구현하는 방법은 다음과 같다:

1. 역방향으로 계산할 때 사용하는 `A_i = δ_i + γλA_{i+1}`는 실제로 다음을 계산한다:
   - A_i = δ_i + γλ(δ_{i+1} + γλδ_{i+2} + ...)
   - = δ_i + γλδ_{i+1} + (γλ)²δ_{i+2} + ...

2. 따라서 거듭제곱 형태의 감쇄(`(γλ)^i`)를 명시적으로 작성할 필요가 없다. 재귀적인 계산 방식 자체가 각 단계마다 γλ를 누적 곱하므로, 자연스럽게 미래 시점의 TD 오류일수록 더 큰 감쇄(더 높은 거듭제곱의 γλ)가 적용된다.

3. 예를 들어 버퍼의 마지막 인덱스가 T라면:
   - A_T = δ_T (마지막 상태)
   - A_{T-1} = δ_{T-1} + γλA_T = δ_{T-1} + γλδ_T
   - A_{T-2} = δ_{T-2} + γλA_{T-1} = δ_{T-2} + γλδ_{T-1} + (γλ)²δ_T
   
   이렇게 계산되어, 각 상태의 어드밴티지는 자신의 TD 오류와 미래 TD 오류들의 감쇄된 합이 된다.
