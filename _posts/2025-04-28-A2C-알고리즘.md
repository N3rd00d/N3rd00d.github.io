---
title: "A2C 알고리즘"
description: "A2C(Advantage Actor-Critic) 알고리즘은 정책 그래디언트 기법과 가치 함수 근사를 결합한 강화학습 알고리즘이다. "
date: 2025-04-28T13:45:27.832Z
tags: ["ActorCritic","a2c","강화학습","기계학습","딥러닝","인공지능","정책최적화"]
---
# A2C 알고리즘

A2C(Advantage Actor-Critic) 알고리즘은 정책 그래디언트 기법과 가치 함수 근사를 결합한 강화학습 알고리즘이다. 이 알고리즘은 [Policy Gradient Theorem](https://velog.io/@sh41107/Policy-Gradient-Theorem)을 기반으로 하며, [REINFORCE 알고리즘](https://velog.io/@sh41107/REINFORCE-알고리즘)의 높은 분산 문제를 개선하고, 심층신경망의 파라미터($\theta$)를 조정하여 정책을 최적화한다.

## REINFORCE의 한계와 A2C

REINFORCE와 같은 기존 정책 그래디언트 알고리즘에는 두 가지 주요 단점이 있다:

- 그래디언트 추정의 분산이 높아 학습이 불안정하다.
- 에피소드가 완전히 끝나야만 정책을 업데이트할 수 있다.

A2C는 이러한 한계를 개선한다.

## Actor-Critic 구조

A2C는 두 개의 주요 구성 요소로 이루어져 있다:

1. **Actor(정책 네트워크)**: 주어진 상태에서 행동을 선택하는 정책 $\pi_\theta(a|s)$를 학습한다.
2. **Critic(가치 네트워크)**: 상태의 가치를 평가하는 가치 함수 $V_\phi(s)$를 학습한다.

이러한 구조의 특징:

- Actor는 Critic의 평가를 바탕으로 정책을 개선한다.
- Critic은 실제 경험을 통해 상태 가치를 추정한다.
- 두 네트워크의 상호작용으로 학습 안정성이 향상된다.

## Advantage 함수

A2C 알고리즘의 핵심은 Advantage 함수 $A(s,a)$를 사용하는 것이다. Advantage 함수는 특정 행동이 평균적인 행동보다 얼마나 더 좋은지(또는 나쁜지) 측정하는 지표이다.

$$
A(s,a) = Q(s,a) - V(s)
$$

- $Q(s,a)$: 상태 $s$에서 행동 $a$를 취한 후의 기대 리턴
- $V(s)$: 상태 $s$의 기대 가치

Advantage 함수는 다음과 같이 근사할 수 있다:

$$
A(s_t,a_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)
$$

이 표현식은 시간차 오차(Temporal Difference error, TD error)라고도 하며, 현재 상태에서 취한 행동의 상대적 가치를 나타낸다.

## 정책 네트워크 업데이트

A2C 알고리즘에서 정책 파라미터 $\theta$의 업데이트는 다음 수식에 따라 이루어진다:

$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ A(s,a) \nabla_\theta \log \pi_\theta(a|s) \right]
$$

이는 REINFORCE 알고리즘과 유사하나, 중요한 차이점은 리턴 $G_t$ 대신 Advantage 함수 $A(s,a)$를 사용한다는 점이다. 이를 통해 그래디언트 추정의 분산을 줄일 수 있다.

실제 구현에서 정책 그래디언트 업데이트는 다음과 같다:

$$
\theta \leftarrow \theta + \alpha \sum_t A(s_t,a_t) \nabla_\theta \log \pi_\theta(a_t|s_t)
$$

## 가치 네트워크 업데이트

A2C에서 Critic 네트워크는 시간차(TD) 학습을 통해 가치 함수를 갱신한다. TD 학습의 특징:

- 에피소드 종료를 기다리지 않고 매 스텝마다 가치 함수를 갱신한다.
- 실시간 학습으로 샘플 효율성을 높인다.

Critic 네트워크의 파라미터 $\phi$는 다음과 같이 업데이트된다:

$$\phi \leftarrow \phi - \beta \nabla_\phi \sum_t \left( V_\phi(s_t) - (r_t + \gamma V_\phi(s_{t+1})) \right)^2$$

여기서 $\beta$는 Critic의 학습률이고, 목표는 예측된 가치 $V_\phi(s_t)$와 TD 타겟 $r_t + \gamma V_\phi(s_{t+1})$ 사이의 제곱 오차를 최소화하는 것이다.

## A2C의 적용 분야

A2C 알고리즘은 다양한 강화학습 과제에 적용할 수 있다:

- 연속적인 행동 공간을 가진 제어 문제
- 장기적인 보상 구조를 가진 환경
- 샘플 효율성이 중요한 문제

A2C의 성능을 향상시키기 위해 GAE(Generalized Advantage Estimation)와 같은 기법을 결합하여 사용할 수 있다.
