---
title: "PPO 알고리즘"
description: "PPO(Proximal Policy Optimization) 알고리즘은 2017년 OpenAI에서 발표한 정책 최적화 알고리즘이다. "
date: 2025-05-05T07:20:24.146Z
tags: ["OpenAI","PPO","강화학습","딥러닝","머신러닝","인공지능","정책최적화","중요도샘플링","클리핑매커니즘"]
thumbnail: /images/741e9d44-20a8-4ca6-82bd-e33d2156e65e-image.png
---
# PPO 알고리즘

PPO(Proximal Policy Optimization) 알고리즘은 2017년 OpenAI에서 발표한 정책 최적화 알고리즘이다. PPO는 TRPO의 복잡한 계산을 단순화하면서도 성능을 유지하여, 현재 가장 널리 사용된다. 

## 중요도 샘플링(Importance Sampling)의 활용

### 중요도 샘플링의 개념과 필요성

중요도 샘플링은 한 확률 분포(q)에서 얻은 샘플을 사용하여 다른 확률 분포(p)에 대한 기대값을 추정하는 통계적 기법이다. 기본 아이디어는 다음과 같다:

$$\mathbb{E}_{p}[f(x)] = \int f(x) p(x) \, dx = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx = \mathbb{E}_{q} \left[ f(x) \frac{p(x)}{q(x)} \right]$$

여기서 $\frac{p(x)}{q(x)}$는 중요도 가중치(importance weight)라고 부른다.

### PPO에서의 중요도 샘플링 적용

PPO에서는 정책 그래디언트 계산 시 중요도 샘플링을 적용한다. 구체적으로, 이전 정책 $\pi_{\theta_{old}}$로 수집한 상태-행동 샘플을 사용하여 새로운 정책 $\pi_{\theta}$의 성능을 평가할 때 다음 비율을 사용한다:

$$r(\theta) = \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{old}}(a \mid s)}$$

이 비율은 새 정책과 이전 정책의 행동 확률 비율로, 중요도 가중치 역할을 한다. 이를 통해:

1. 새 정책에서 확률이 높아진 행동에는 더 큰 가중치가 부여된다($r(\theta) > 1$).
2. 새 정책에서 확률이 낮아진 행동에는 더 작은 가중치가 부여된다($r(\theta) < 1$).

예를 들어, 이전에는 '왼쪽으로 이동' 행동을 10% 확률로 선택했지만, 새 정책에서는 20% 확률로 선택한다면, 이 행동의 중요도 가중치는 2.0이 된다. 이는 현재 정책하에서 이 행동이 더 자주 선택되었을 것이므로 그 영향력을 두 배로 평가해야 함을 의미한다.

### 중요도 샘플링의 장단점

중요도 샘플링은 이전 정책으로 수집한 데이터를 버리지 않고 재활용할 수 있어 데이터 효율성을 높이고, 적은 데이터로도 효과적인 학습이 가능하며, 다양한 소스의 데이터를 활용해 학습 안정성을 향상시킨다. 그러나 두 정책의 차이가 커질수록 중요도 가중치의 분산이 증가하여 추정 정확도가 떨어지고, 극단적으로 크거나 작은 가중치가 발생할 수 있어 학습이 불안정해지는 단점이 있다. PPO 알고리즘은 이러한 중요도 샘플링의 단점을 클리핑 메커니즘을 통해 효과적으로 해결한다.

## PPO-Clip 알고리즘

PPO의 클리핑 버전(PPO-Clip)은 목적 함수에 클리핑 연산자를 도입하여 정책 업데이트의 폭을 직접 제한하는 방식이다. 목적 함수는 다음과 같다:

$$L^{CLIP}(\theta) = \hat{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$

여기서:
- $r_t(\theta)$: 새 정책과 이전 정책의 확률 비율 $\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)}$
- $\hat{A}_t$: 행동의 어드밴티지 추정값
- $\epsilon$: 클리핑 하이퍼파라미터 (일반적으로 0.1 또는 0.2)

클리핑 매커니즘은 정책 업데이트의 폭을 제한하여 중요도 비율 $r(\theta)$가 극단적 값을 갖는 것을 방지한다. 이는 어드밴티지 값의 부호에 따라 다르게 작용한다.

**1. 긍정적 어드밴티지(A > 0)일 때**: 좋은 행동의 확률을 과도하게 증가시키지 않도록 제한한다. $r(\theta) > 1+\epsilon$이면, $r(\theta)$를 $1+\epsilon$으로 클리핑하여 정책 변화를 제한한다. 예를 들어, 기존 30% 확률의 좋은 행동을 90%로 급격히 높이려는 경우($r(\theta) = 3.0$), $\epsilon = 0.2$일 때 최대 36%로만 증가하도록 제한한다.
**2. 부정적 어드밴티지(A < 0)일 때**: 나쁜 행동의 확률을 과도하게 감소시키지 않도록 제한한다. $r(\theta) < 1-\epsilon$이면, $r(\theta)$를 $1-\epsilon$으로 클리핑한다. 이때 목적 함수 $\min(r(\theta)\hat{A}, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A})$는 더 작은 값을 선택하므로, 나쁜 행동의 확률을 급격히 낮추려는 시도를 제한한다.
이러한 클리핑 메커니즘은 급격한 정책 변화를 방지하여 학습 안정성을 크게 향상시킨다.

## PPO의 구현 세부사항

### 1. 어드밴티지 정규화

$$\hat{A}_t^{norm} = \frac{\hat{A}_t - \mu_{\hat{A}}}{\sigma_{\hat{A}}}$$

여기서:
- $\hat{A}_t$: 시간 스텝 $t$에서의 원래 어드밴티지 추정값
- $\mu_{\hat{A}}$: 현재 배치의 모든 어드밴티지 값의 평균 (배치 평균)
- $\sigma_{\hat{A}}$: 현재 배치의 모든 어드밴티지 값의 표준편차 (배치 표준편차)

이 정규화는 통계학에서 표준화(standardization)라고 부르는 과정으로, 어드밴티지 값들의 분포를 평균 0, 표준편차 1인 정규분포 형태로 변환한다. 이 정규화 과정은 PPO 알고리즘의 성능을 크게 향상시키는 것으로 알려져 있으며, 대부분의 PPO 구현에서 표준적으로 사용된다.

### 2. 가치 함수 손실과 엔트로피 보너스

PPO는 주로 다음 세 가지 손실 함수를 결합하여 사용한다:

1. **클리핑된 정책 손실**: $L^{CLIP}(\theta)$
2. **가치 함수 손실**: $L^{VF}(\theta) = (V_\theta(s_t) - V_{target})^2$
3. **엔트로피 보너스**: $S[\pi_\theta](s_t)$

최종 손실 함수는:

$$L^{TOTAL}(\theta) = \hat{E}_t[L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_{\theta}](s_t)]$$

여기서 $c_1$과 $c_2$는 각 항의 상대적 중요도를 조절하는 계수이다.

## 마무리

PPO 알고리즘은 간단한 구현, 우수한 성능, 안정적인 학습 특성으로 인해 현대 강화학습에서 가장 인기 있는 알고리즘 중 하나이다. 클리핑 메커니즘은 정책 업데이트를 효과적으로 제한하여 안정성을 제공하고, 중요도 샘플링을 통한 오프-폴리시 학습은 데이터 효율성을 높인다.
