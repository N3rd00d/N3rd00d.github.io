---
title: "엔트로피 정규화"
description: "강화학습의 핵심 과제 중 하나는 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 찾는 것이다. 에이전트는 이미 알고 있는 좋은 전략을 활용하는 동시에, 더 나은 전략을 발견하기 위해 새로운 행동을 탐색해야 한다. "
date: 2025-04-29T00:53:44.780Z
tags: ["PPO","a2c","강화학습","엔트로피","엔트로피정규화","정책최적화","탐색전략"]
---
# 엔트로피 정규화

강화학습의 핵심 과제 중 하나는 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 찾는 것이다. 에이전트는 이미 알고 있는 좋은 전략을 활용하는 동시에, 더 나은 전략을 발견하기 위해 새로운 행동을 탐색해야 한다. 이 글에서는 엔트로피를 활용하여 탐색을 장려하는 방법에 대해 알아본다.

## 정책 엔트로피의 개념

정보이론에서 엔트로피는 불확실성 또는 랜덤성의 척도다. 강화학습 맥락에서 정책 엔트로피는 에이전트의 정책이 얼마나 확률적인지, 즉 행동 선택이 얼마나 무작위적인지를 측정한다.

정책 엔트로피의 수학적 정의는 다음과 같다:

$$
H(\pi(·\mid s)) = -\sum_a \pi(a\mid s) \log \pi(a\mid s)
$$

여기서:
- $\pi(a\mid s)$는 상태 $s$에서 행동 $a$를 취할 0~1사이의 확률값
- $H(\pi(·\mid s))$는 상태 $s$에서의 정책 엔트로피
- $\log \pi(a\mid s)$는 항상 0 이하임으로 음수를 곱해 엔트로피가 항상 0 이상의 값을 갖도록 함

## 엔트로피 값의 의미

정책 엔트로피의 값은 정책의 확률 분포 특성에 따라 달라진다:

1. **높은 엔트로피**:
   - 모든 행동에 대해 균일하게 확률이 분포된 경우 (완전 무작위 정책)
   - 최대 엔트로피는 $\log(\lvert A \rvert)$이며, $\lvert A \rvert$는 가능한 행동의 수
   - 적극적인 탐색(exploration) 단계를 의미

2. **낮은 엔트로피**:
   - 특정 행동에 높은 확률이 집중된 경우 (결정적 정책)
   - 엔트로피가 0에 가까우면 거의 결정적인 정책
   - 활용(exploitation) 중심의 전략을 의미

2개의 행동(이진 선택)이 있는 환경에서의 엔트로피 값 예시:

- 균일한 분포: $p(a_1) = p(a_2) = 0.5$
  - $H(\pi) = -(0.5 \log 0.5 + 0.5 \log 0.5) \approx 0.693$
- 편향된 분포: $p(a_1) = 0.8, p(a_2) = 0.2$
  - $H(\pi) = -(0.8 \log 0.8 + 0.2 \log 0.2) \approx 0.500$
- 결정적: $p(a_1) = 0.99, p(a_2) = 0.01$
  - $H(\pi) = -(0.99 \log 0.99 + 0.01 \log 0.01) \approx 0.056$

## 엔트로피 정규화

엔트로피 정규화는 강화학습 알고리즘의 목적 함수에 정책 엔트로피 항을 추가하여 탐색을 장려하는 기법이다. 기본 형태는 다음과 같다:

$$
J_{\text{regularized}}(\theta) = J(\theta) + \beta H(\pi_\theta)
$$

여기서:
- $H(\pi_\theta)$는 정책의 평균 엔트로피
- $\beta$는 엔트로피의 중요도를 조절하는 하이퍼파라미터

## A2C에서의 엔트로피 정규화

A2C(Advantage Actor-Critic) 알고리즘에서 엔트로피 정규화를 적용한 정책 그래디언트는 다음과 같다:

$$
\nabla_\theta J_{\text{regularized}}(\theta) = \mathbb{E} \left[ A(s,a) \nabla_\theta \log \pi_\theta(a\mid s) \right] + \beta \nabla_\theta H(\pi_\theta)
$$

이 식을 실제 구현에서는 다음과 같이 적용한다:

$$
\theta \leftarrow \theta + \alpha \left( \sum_t A(s_t,a_t) \nabla_\theta \log \pi_\theta(a_t\mid s_t) + \beta \nabla_\theta \sum_t H(\pi_\theta(·\mid s_t)) \right)
$$

엔트로피 항은 정책이 다양한 행동을 탐색하도록 유도하며, $\beta$ 값을 통해 탐색의 정도를 조절할 수 있다.


## 엔트로피 계산 예시

```python
# 정책 네트워크에서 행동 확률 분포를 얻음
action_probs = policy_network(state)
# 분포의 엔트로피 계산
entropy = -torch.sum(action_probs * torch.log(action_probs), dim=1)
# 배치의 평균 엔트로피
mean_entropy = entropy.mean()
# 손실 함수에 엔트로피 보너스 추가
loss = policy_loss - beta * mean_entropy
```
